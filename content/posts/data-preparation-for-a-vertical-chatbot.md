---
title: "我做了一个垂直应用的 Chatbot：关于数据准备的一些经验"
date: 2026-01-04T17:27:00+08:00
draft: true
tags: ["AI", "数据工程"]
---

这篇文章记录的是：我在为一款 **垂直应用 Chatbot** 准备数据的过程中，经过一系列实践尝试，逐渐沉淀下来的一套数据工作流思路。

它并不新颖，也并不复杂，但在真实的业务环境中，非常管用。

---

## 适合谁阅读（请先看这里）

在继续往下之前，我想先明确一点：这篇文章**并不适合所有人**。

**它可能不太适合：**
- 算法研究员
- AI 竞赛玩家
- 以模型结构与训练技巧为主要关注点的 ML 工程师

**它更适合：**
- AI 产品经理
- 应用型 AI 工程师
- 在团队里「被迫开始做数据」的人
- 想把 AI 真正跑起来，而不仅仅停留在 Demo 阶段的人

如果你期待的是更复杂的模型、更新颖的算法，这篇文章大概率会让你失望；  
但如果你关心的是，**在现实约束下，数据如何一步步从无到有，从充满噪声到变得可用**，那它或许能给你提供一些参考。

---

## 任务背景：做一款面向车主的 Chatbot

最初的目标，是做一款**面向普通车主的问答型 Chatbot**，用于解答他们在真实用车场景中会遇到的各类问题。

例如：

- 「仪表盘上那个像茶壶一样的灯亮了，是怎么回事？」
- 「开车时方向盘老是抖，速度越快抖得越厉害，怎么回事？」
- 「这两天仪表盘上老显示一个保养的黄扳手，请问再多跑一千到二千公里去保养行吗？」

这些问题通常具有几个共同特征：

- **高度口语化**
- **上下文信息缺失**
- **专业名词与俗称混杂**
- **带有明显的情绪与不确定性**

为了让模型的回复更贴近真实车主的表达方式，在语气上显得亲切，在判断上让人感到靠谱，而不只是一个干巴巴的“百科式问答机器人”，  
我需要准备一批**来源于真实车主、表达自然、质量可控且规模足够大的提问数据**，用于后续的车主画像分析与模型微调。

---

## 第一个问题：数据从哪里来？

无论是用于后续的模型 LoRA 微调，还是支撑更高质量的车主画像分析，以及多轮对话策略设计，我们首先要解决的，都是同一个问题：**需要大量来自真实车主表达的问答数据**。

但在现有的数据资产中，并不具备这一类型的数据，因此只能考虑从外部渠道入手，尽可能补齐这一数据基础。

在不依赖私有数据、也不触碰合规边界的前提下，可行的数据来源其实并不多，主要包括：

- 汽车论坛（汽车之家、懂车帝等）
- 短视频平台（B 站、抖音、小红书等）
- 公开数据集（竞赛数据集、Hugging Face 等）
- AI 生成的 dummy 数据

综合数据质量、表达真实性与规模潜力等因素评估后，我们最终将最高优先级放在了**短视频平台（B 站、抖音、小红书等）**。 

这类平台上的内容更贴近真实车主的自然表达，往往带有情绪、口语化表述与大量俗称，同时数据规模也足够大，更有利于后续的数据扩展与迭代。

但关键问题在于：**这些地方的数据，几乎都是“极度嘈杂的”。**

- 大量与问题无关的闲聊与互动（如点赞、调侃）
- 大量重复内容、吐槽式表达
- 大量上下文缺失的碎片文本
- 甚至夹杂着广告或完全无关的信息

很快，我便意识到一件事：

> **真正的难点，并不在于“能不能抓到数据”，  
> 而在于“如何从一堆噪声中，筛选出少量真正可用的内容”。**

---

## 为什么我没有一开始就用大模型？

在这个阶段，很多人会下意识地给出一种方案：  
> 直接把数据丢给大模型，让大模型逐条判断“这是不是一个高质量的车主问题”。

但在早期实践中，我刻意没有这么做，原因很简单：**成本不可控**。

从公开平台获取的数据规模往往在数十万级别，而单条文本的 token 数也并不低。如果将全量数据直接交由大模型进行判断，token 消耗会迅速膨胀，整体成本在这个阶段并不友好，也缺乏可持续性。

更重要的是，在数据极脏、清洗目标尚未稳定之前，过早引入大模型，反而会放大不确定性。

因此，在这一阶段，我更需要的是一种**稳定、可控、且成本可预期的筛选方式**，用来先建立数据处理的基本秩序。

---

## 一个非常“老”的组合：TF-IDF + Logistic Regression

在正式确定方案之前，我也曾考虑过一些看起来“更现代”的做法。

例如，使用 [SetFit](https://github.com/huggingface/setfit)，通过少量标注样本进行 few-shot 文本分类，对全量文本数据进行筛选。

但最终，在使用 ChatGPT 对不同方案进行初步梳理与对比评估之后，我选择了一套更加朴素、也更加稳妥的方法：**TF-IDF + Logistic Regression**。

- 使用 **TF-IDF** 对文本进行向量化表示  
- 使用 **Logistic Regression** 进行文本二分类  
- 目标并不是“判断是否完美”  
- 而是**尽可能把明显无用的数据挡在外面**

这套方法的优势在于：

- 成本极低，本地即可轻量运行  
- 运行速度快，适合对大规模数据进行清洗与筛选
- 对中文短文本场景表现稳定 

它并不聪明，但在这个阶段，**足够可靠**。

---

## 接下来要解决的，是“怎么把数据工作流跑起来”

在确定了整体的数据筛选思路之后，接下来要解决的问题，就变得非常具体了：  
**如何把这些判断，落实为一条能够稳定运行的数据工作流。**

这一阶段，我主要做了四件事情。

### 1. 原始数据的获取：从公开平台爬取真实车主表达

首先需要解决的，是原始数据从哪里来。

在不依赖私有数据、也不触碰合规边界的前提下，我们选择了使用开源工具 [MediaCrawler](https://github.com/NanmiCoder/MediaCrawler)，从多个公开平台中爬取与汽车相关的评论与问答内容，作为最初的原始数据来源。

搜索关键词包含常见车型的常见故障，以及汽车各配件的常见故障等。

需要说明的是，这一步获取的数据本身质量并不高，噪声极大，更多是作为一个**原始语料池**，而不是可直接使用的训练数据。

### 2. 用简单规则与少量样本，先把数据“分一分”

在原始数据极度嘈杂的情况下，我并没有试图一开始就追求精细判断，而是通过一些基础的清洗脚本，配合少量人工标注样本，先构建最基础的正负例数据。

这一步的目标不是得到一个高精度分类器，而是让机器能够**在大规模数据中，先帮我过滤掉一部分明显无效的内容**，从而把后续需要人工关注的数据量控制在一个可接受的范围内。

### 3. 扩充数据上下文，让人工判断变得可行

在实际清洗过程中，我很快发现，仅有孤立的问题文本，往往不足以支撑有效判断。

因此，在原始文本之外，我额外补充了一些能够帮助理解上下文的信息，例如通过评论 ID 反向获取对应的视频 ID 与原始内容链接，方便在需要时回溯完整语境，进行人工分析与判断。

这一步并不直接提升模型效果，但**显著提升了人工参与数据筛选时的效率与准确性**。

### 4. 稳定数据流程，并逐步扩大数据规模

在流程能够跑通之后，我并没有急着引入更复杂的模型，而是优先保证这套清洗与筛选流程本身足够稳定、可重复执行。

在此基础上，通过多轮迭代，逐步扩展数据来源与数据规模，让数据质量与数量在可控的节奏下同步提升。

对我而言，这一步的核心目标只有一个：  
**让数据准备这件事，从一次性的工作，变成一条可以长期运行的流程。**

---

## 写在最后

很多 AI 项目失败，并不是模型本身不够强，而是从一开始，数据就没准备好。

> **Garbage in, garbage out.**

如果你正站在一个“该用什么方法开始准备数据”的节点上，或许这些经验能为你提供一些参考。